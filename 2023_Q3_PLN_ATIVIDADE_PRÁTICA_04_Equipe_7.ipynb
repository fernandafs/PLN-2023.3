{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 04 [Uso da API da OpenAI com técnicas de PLN]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 04** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/GzwCq3R7ExtE9g9a8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 20/11 (segunda-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "- Fernanda Felix da Silva RA: 11201921613\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "- Ricardo Juniti Kawai RA: 11202021571\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "- Samuel Brito da Silva RA: 11201810515"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: `\n",
        "\n",
        "- Cápitulo 7 (Link): https://brasileiraspln.com/livro-pln/1a-edicao/parte4/cap7/cap7.html\n",
        "\n",
        "\n",
        "`Segundo capítulo:`\n",
        "\n",
        "- Cápitulo 19 (Link): https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap19/cap19.html\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso da **API da OpenAI** aplicando, no mínimo, 3 técnicas de PLN. As técnicas devem ser aplicadas nos 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        ">\n",
        "\n",
        "**RESTRIÇÃO**: É obrigatório usar o *endpoint* \"*`Chat Completions`*\".\n",
        "\n",
        ">\n",
        "\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   **Similaridade de Textos**\n",
        "*   **Reconhecimento de Entidades Nomeadas**\n",
        "*   **Sistemas de Perguntas e Respostas**\n",
        "\n",
        ">\n",
        "\n",
        "Os capítulos devem ser os mesmos selecionados na **ATIVIDADE PRÁTICA 02**. Para consultar os capítulos, considere a seguinte planilha:\n",
        "\n",
        ">\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC. Não é permitido alterar os capítulos já selecionados.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação as técnicas usadas e a criatividade envolvida na aplicação das mesmas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1.0 - Instalação do pacote Python da API da OpenAI\n",
        "\n",
        "!pip install openai==0.28.1"
      ],
      "metadata": {
        "id": "AwyHaQQKVCJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b432e5-1c4e-45d4-e9b5-536d5c26045a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1.1 - Import das bibliotecas utilizadas\n",
        "\n",
        "import openai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from google.colab import files\n",
        "import random"
      ],
      "metadata": {
        "id": "3kW4mKGmQZUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1.2 - Configuração da chave de acesso da API a partir do upload de um arquivo de texto\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H74OOIQ5QdHn",
        "outputId": "b382bfbd-e958-4c53-c069-f6d8411def23"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1.3 - Extração do conteúdo dos capítulos escolhidos + implantação das técnicas de PLN\n",
        "\n",
        "def extrai_texto(link, user_list):\n",
        "\n",
        "  response = requests.get(link, headers={'User-Agent': random.choice(user_list)})\n",
        "\n",
        "  # Valida se a solicitação foi bem sucedida e caso seja, transforma retira os\n",
        "  # paragráfos de texto dentro da página e também inicializa um array para\n",
        "  #guardar os erros.\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    paragrafos = soup.find_all('p')\n",
        "\n",
        "\n",
        "    # Retirada de URL das validações\n",
        "    padrao_url = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "    texto_capitulo = []\n",
        "    paragrafos_coletados = 0\n",
        "\n",
        "    # Passo 1.3.1 - Extração de conteúdo dos arquivos\n",
        "    for paragrafo in paragrafos:\n",
        "      texto = paragrafo.get_text()\n",
        "      texto_sem_urls = re.sub(padrao_url, '', texto)\n",
        "      texto_capitulo.append(texto_sem_urls)\n",
        "\n",
        "      paragrafos_coletados += 1\n",
        "      if paragrafos_coletados >= 35 and link == 'https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap19/cap19.html':\n",
        "        break  # Para de coletar parágrafos após atingir a quantidade desejada (necessário devido a API não conseguir ler o texto inteiro)\n",
        "      elif paragrafos_coletados >= 70:\n",
        "        break # Para de coletar parágrafos após atingir a quantidade desejada (necessário devido a API não conseguir ler o texto inteiro)\n",
        "\n",
        "\n",
        "    texto_completo = '\\n'.join(texto_capitulo)\n",
        "    # Passo 1.3.2 - Implementação das técnicas de PLN\n",
        "\n",
        "    endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "    # 1.3.2.1 Correção Gramatical:\n",
        "    mensagem_sistema = 'Corrija os erros gramaticais no texto.'\n",
        "    mensagem_assistente = 'Mostre a quantidade de palavras/orações corrigidas além de escrever a frase correta.'\n",
        "\n",
        "    parametros = {\n",
        "      \"model\": \"gpt-3.5-turbo-0613\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "          {\"role\": \"user\", \"content\": texto_completo},\n",
        "          {\"role\": \"assistant\", \"content\": mensagem_assistente}]\n",
        "    }\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "    }\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    resultado = resposta.json()\n",
        "\n",
        "    print(\"Resultado Correção Gramatical:\")\n",
        "    print(resultado)\n",
        "\n",
        "    # 1.3.2.2 Sumarização do texto:\n",
        "    mensagem_sistema2 = 'Resuma o texto em poucas palavras.'\n",
        "    mensagem_assistente2 = 'Escreva o texto resumido e indique em quantos paragrafos a menos do original o resumo foi escrito.'\n",
        "\n",
        "    parametros = {\n",
        "      \"model\": \"gpt-3.5-turbo-0613\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": mensagem_sistema2},\n",
        "          {\"role\": \"user\", \"content\": texto_completo},\n",
        "          {\"role\": \"assistant\", \"content\": mensagem_assistente2}]\n",
        "    }\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "    }\n",
        "\n",
        "    resposta2 = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    resultado2 = resposta2.json()\n",
        "\n",
        "    print(\"Resultado Sumarização:\")\n",
        "    print(resultado2)\n",
        "\n",
        "    # 1.3.2.2 Detecção de palavras-chave:\n",
        "    mensagem_sistema3 = 'Gerar palavras-chave de cauda longa referentes ao texto que sejam as melhores para atrair o publico alvo: \"Alunos de PLN\".'\n",
        "    mensagem_assistente3 = 'Escreva as palavras-chave.'\n",
        "\n",
        "    parametros = {\n",
        "      \"model\": \"gpt-3.5-turbo-0613\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": mensagem_sistema3},\n",
        "          {\"role\": \"user\", \"content\": texto_completo},\n",
        "          {\"role\": \"assistant\", \"content\": mensagem_assistente3}]\n",
        "    }\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "    }\n",
        "\n",
        "    resposta3 = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    resultado3 = resposta3.json()\n",
        "\n",
        "    print(\"Resultado Palavras-chave:\")\n",
        "    print(resultado3)\n",
        "\n",
        "    # 1.3.2.2 Tradução para o inglês:\n",
        "    mensagem_sistema4 = 'Traduza o texto para o inglês, e resuma em poucas palavras.'\n",
        "    mensagem_assistente4 = 'Escreva o texto final.'\n",
        "\n",
        "    parametros = {\n",
        "      \"model\": \"gpt-3.5-turbo-0613\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": mensagem_sistema4},\n",
        "          {\"role\": \"user\", \"content\": texto_completo},\n",
        "          {\"role\": \"assistant\", \"content\": mensagem_assistente4}]\n",
        "    }\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "    }\n",
        "\n",
        "    resposta4 = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    resultado4 = resposta4.json()\n",
        "\n",
        "    print(\"Resultado Tradução inglês resumido:\")\n",
        "    print(resultado4)\n",
        "  else:\n",
        "\n",
        "    print(f\"Não foi possível obter o conteúdo do link: {link}\")\n",
        "    return []\n",
        "links = [\n",
        "    \"https://brasileiraspln.com/livro-pln/1a-edicao/parte4/cap7/cap7.html\",\n",
        "    \"https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap19/cap19.html\"\n",
        "]\n",
        "\n",
        "user_agents_list = [\n",
        "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36'\n",
        "]\n",
        "\n",
        "# Perpassa por ambos os links chamando a função \"extrai_texto\" para obter seu conteúdo.\n",
        "for link in links:\n",
        "    print(f\"Extraindo capítulo: {link}\")\n",
        "    extrai_texto(link, user_agents_list)"
      ],
      "metadata": {
        "id": "RyUailD5vi9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae58ce39-b039-4251-8a7d-960672dc437f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraindo capítulo: https://brasileiraspln.com/livro-pln/1a-edicao/parte4/cap7/cap7.html\n",
            "Resultado Correção Gramatical:\n",
            "{'id': 'chatcmpl-8LK775fHK67t53k7583sHwW2x5GEu', 'object': 'chat.completion', 'created': 1700092897, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'No texto fornecido, foram corrigidos os seguintes erros gramaticais:\\n\\n1. Na frase \"Para isso, consideramos a classe de cada palavra, sua ordem na sentença e sua relação com as outras palavras.\", a palavra \"consideramos\" deveria estar na terceira pessoa do plural para concordar com \"nós\". A frase correta seria: \"Para isso, consideramos a classe de cada palavra, sua ordem na sentença e sua relação com as outras palavras.\"\\n\\n2. Na frase \"Neste Capítulo, vamos conhecer tipos de parsing sob a perspectiva computacional, juntamente com ferramentas e recursos disponíveis para o processamento do português brasileiro.\", a palavra \"parsing\" é um anglicismo inadequado e deveria ser substituída por \"análise\". A frase correta seria: \"Neste Capítulo, vamos conhecer tipos de análise sob a perspectiva computacional, juntamente com ferramentas e recursos disponíveis para o processamento do português brasileiro.\"\\n\\n3. Na frase \"A tarefa de parsing consiste em, dada uma entrada com uma sentença sem nenhuma anotação (raw), um modelo faz uma predição da estrutura sintática dessa sentença.\", a expressão \"uma entrada com uma sentença sem nenhuma anotação (raw)\" está mal construída. A expressão correta seria: \"dada uma entrada de uma sentença sem nenhuma anotação (raw)\", para indicar que a entrada é \"de\" uma sentença sem anotação.\\n\\n4. Na frase \"Assim, de acordo com o tipo de análise sintática adotada, há parsers de constituência e parsers de dependência.\", a palavra \"parsers\" está repetida desnecessariamente. A frase correta seria: \"Assim, de acordo com o tipo de análise sintática adotada, há parsers de constituência e dependência.\"\\n\\n5. Na frase \"Mas há uma perspectiva adicional sob a qual podemos caracterizar tipos de parsing e parsers: trata-se do escopo ou profundidade com que a análise sintática é executada.\", a palavra \"parsers\" está repetida desnecessariamente. A frase correta seria: \"Mas há uma perspectiva adicional sob a qual podemos caracterizar tipos de parsing: trata-se do escopo ou profundidade com que a análise sintática é executada.\"\\n\\n6. Na frase \"O primeiro tipo é denominado deep (em português, profundo) ou parsing completo e o segundo tipo é denominado shallow (em português, superficial) ou parsing parcial.\", a palavra \"parsing\" está repetida desnecessariamente. A frase correta seria: \"O primeiro tipo é denominado deep (em português, profundo)'}, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 3489, 'completion_tokens': 609, 'total_tokens': 4098}}\n",
            "Resultado Sumarização:\n",
            "{'id': 'chatcmpl-8LK8DmrMuR1DDYRNkHd1BwT0Lg7Qx', 'object': 'chat.completion', 'created': 1700092965, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'A análise sintática é um nível de análise linguística que examina os padrões de estruturação de sentenças. Existem diferentes tipos de parsing, como o de constituência e o de dependência, que podem ser realizados de forma completa ou parcial. Existem diversas ferramentas e recursos disponíveis para o processamento do português brasileiro, como os corpus anotados, as bibliotecas de PLN (Processamento de Linguagem Natural) e as ferramentas de parsing, como o Curupira e o Parser LX. O texto foi resumido em 3 parágrafos do original.'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3494, 'completion_tokens': 136, 'total_tokens': 3630}}\n",
            "Resultado Palavras-chave:\n",
            "{'id': 'chatcmpl-8LK8TOR6BKBMmfNtliHAhmyPeDTGV', 'object': 'chat.completion', 'created': 1700092981, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'alunos PLN, análise sintática, parsing, parsers de constituência, parsers de dependência, deep parsing, shallow parsing, chunking, treebank, corpus anotado, Bosque, CINTIL, PetroGold, Porttinari, Veredas, Curupira, Donatus, PassPort, spaCy, Stanza, NLTK, Parser LX, Parser VISL.'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3501, 'completion_tokens': 83, 'total_tokens': 3584}}\n",
            "Resultado Tradução inglês resumido:\n",
            "{'id': 'chatcmpl-8LK8b892ERFEqiBNbguHAKFlcYfUV', 'object': 'chat.completion', 'created': 1700092989, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Elisa Terumi Rubel Schneider \\nAdriana S Pagano \\nAna Clara S Pagano \\n26/09/2023\\nPDF\\nSyntax is the level of linguistic analysis in which we examine the patterns of sentence structure. That is, we analyze how words are organized into units that convey meaning within the sentence. To do this, we consider the class of each word, its order in the sentence, and its relationship with the other words. As seen in Chapter 6, in NLP, the computational analysis performed at the syntactic level is called parsing, the tool that performs this task is called a parser, and the resource created through syntactic analysis is called a treebank.\\n\\nIn this chapter, we will learn about types of parsing from a computational perspective, along with tools and resources available for processing Brazilian Portuguese. The task of parsing consists of given an input with an unannotated sentence (raw), a model makes a prediction of the syntactic structure of that sentence. As we saw in Chapter 6, the goal of syntactic processing is to identify units (such as words, phrases, and clauses) in the sentence and establish grammatical relations between them in order to extract some type of information. These relations can be analyzed in terms of:\\n\\nThus, depending on the type of syntactic analysis adopted, there are constituency parsers and dependency parsers.\\n\\nBut there is an additional perspective under which we can characterize types of parsing and parsers: this is the scope or depth with which syntactic analysis is performed. In this sense, we can analyze sentences exhaustively until we obtain a complete analysis of their structure or make a shallower analysis to obtain an analysis with minimal but relevant information for NLP tasks.\\n\\nThe first type is called deep or complete parsing, and the second type is called shallow or partial parsing. However, it should be noted that this terminology may vary. In general use, parsing and parser have come to refer to complete parsing. Partial parsing is known as chunking, and the tool as a chunker, although chunking is just one of several approaches to implementing partial parsing (Jurafsky; Martin, 2023).\\n\\nBoth constituency parsing and dependency parsing can be performed in a complete or partial manner. Taking constituency parsing as an example, complete or deep analysis extracts all the groupings and syntactic relations in a sentence. For example, given the sentence:\\n\\nExample 7.1\\nEmma Watson posted a photo with Daniel Radcliffe.\\n\\nFigure 7.1 represents a tree diagram that shows the depth of the analysis.\\n\\nA partial analysis, on the other hand, extracts delimited constituents without establishing hierarchy between them or how some are contained within others. Figure 7.2 illustrates the shallow, non-hierarchical analysis of partial parsing for Example 7.1.\\n\\nThe goal of partial parsing is to generate a shallow representation of the sentence structure that allows for faster processing of large volumes of text. It is generally achieved through tokenization of a sentence into words, identification of part-of-speech (PoS) tags, and segmentation into chunks'}, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 3482, 'completion_tokens': 616, 'total_tokens': 4098}}\n",
            "Extraindo capítulo: https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap19/cap19.html\n",
            "Resultado Correção Gramatical:\n",
            "{'id': 'chatcmpl-8LK9SSlXFCpVoIBjOKy0qRG18Lxm9', 'object': 'chat.completion', 'created': 1700093042, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'A Correção Automática de Redação (CAR) é uma das várias aplicações do PLN e pode ser definida como \"o processo de avaliação e atribuição de nota em textos escritos em prosa, via programas computacionais\" (Shermis; Burstein, 2013).\\n\\n- Corrigido\\n\\nA correção manual de redações é uma prática bastante antiga, mas esse processo feito de forma automática data da década de 60, em inglês, e é ainda mais recente para o português.\\n\\n- Corrigido\\n\\nEm inglês, as áreas de Automated Essay Scoring (AES) e Automated Essay Evaluation (AEE) surgem como distintas, porém complementares e, às vezes, com alguma intersecção. A primeira tem como desafio a automatização de atribuição de nota para redação, enquanto a segunda está preocupada, também, em automatizar o retorno ou feedback para o aluno, colaborando para o processo de aprendizagem da escrita.\\n\\n- Corrigido\\n\\nA AES costuma ser traduzida para o português como Avaliação Automática de Redação (AAR) (Bittencourt Jr., 2020; Da Silva Jr., 2021; Lima et al., 2023), enquanto a AEE está associada ao termo Correção Automática de Redação (CAR), apesar do falso cognato. Neste capítulo, adotamos o segundo, por entendermos que ele abarca as duas áreas AEE e AES, ou seja, trata-se de uma solução completa. Para que seja considerada como solução completa de CAR, a aplicação deve contemplar pelo menos três etapas básicas.\\n\\n- Corrigido\\n\\nCada uma dessas etapas pode ser vista como uma aplicação independente no PLN. Por exemplo, existem várias ferramentas de auxílio à escrita, bem como corretores ortográficos e gramaticais, que executam exclusivamente a tarefa de identificação de desvios no texto; e isso constitui uma aplicação em si. Da mesma forma, a tarefa de dar um feedback com sugestões para o aluno é semelhante a outras aplicações de PLN que envolvem geração de linguagem natural (ou Natural Language Generation).\\n\\n- Corrigido\\n\\nApesar de poderem figurar como ferramentas e/ou aplicações independentes, consideramos que a correção de redação, para ser entendida como uma solução completa do ponto de vista pedagógico, exige o cumprimento dessas três etapas, que serão bem detalhadas ao longo deste capítulo.\\n\\n- Corrigido\\n\\nAntes de abordar cada uma das etapas, porém, faremos uma breve explicação sobre o objeto de estudo da CAR, que é a redação escolar, definindo e exemplificando os principais gêneros e tipos textuais, os critérios avaliados e alguns modelos brasileiros de correção de redação.\\n\\n- Corrigido\\n\\nA redação escolar é considerada um gênero textual,'}, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 3420, 'completion_tokens': 678, 'total_tokens': 4098}}\n",
            "Resultado Sumarização:\n",
            "{'id': 'chatcmpl-8LKAcMcbKNNaTQCbR3sG5lGFGyYKH', 'object': 'chat.completion', 'created': 1700093114, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'A correção automática de redação é uma aplicação do processamento de linguagem natural que avalia e atribui notas a textos escritos. O processo automático de correção de redações surgiu na década de 60 para o inglês e mais recentemente para o português. Existem diferentes etapas nesse processo, como a identificação de erros, a geração de feedback e a atribuição de nota. A redação escolar é um gênero textual utilizado para avaliar habilidades de escrita e desenvolvimento de pensamento crítico. Existem diversos tipos e gêneros de redação escolar, que são avaliados de acordo com critérios como uso da norma padrão da língua, abordagem do tema, adequação ao tipo e gênero textual, coerência e coesão. No Brasil, o modelo de correção mais utilizado é o do Enem, seguido por vestibulares como a Fuvest, Unesp e Unicamp. Cada modelo tem seus próprios critérios e faixas de nota. Esses modelos também penalizam falhas graves, como fuga ao tema ou gênero. Além disso, o uso de título na redação pode variar de acordo com cada modelo. [P1]'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3425, 'completion_tokens': 281, 'total_tokens': 3706}}\n",
            "Resultado Palavras-chave:\n",
            "{'id': 'chatcmpl-8LKB8OciHsNdEZvlkLooWu1Ql9OHD', 'object': 'chat.completion', 'created': 1700093146, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '- Correção Automática de Redação\\n- PLN\\n- Avaliação Automática de Redação\\n- Correção Manual de Redações\\n- Redação Escolar\\n- Gêneros Textuais\\n- Tipos Textuais\\n- Correção de Redação\\n- Norma Padrão da Língua Portuguesa\\n- Coesão e Coerência\\n- Desvios Ortográficos e Gramaticais\\n- Proposta de Intervenção\\n- Enem\\n- Vestibulares\\n- Fuvest\\n- Unesp\\n- Unicamp'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3432, 'completion_tokens': 126, 'total_tokens': 3558}}\n",
            "Resultado Tradução inglês resumido:\n",
            "{'id': 'chatcmpl-8LKBMAPHz5E5eVsFfT2OgiONWVdRj', 'object': 'chat.completion', 'created': 1700093160, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Automatic Essay Correction (AEC) is a process of evaluating and grading essays using computer programs. It is a subset of Natural Language Processing (NLP) and has been used since the 1960s for English essays and more recently for Portuguese. AEC encompasses both Automated Essay Scoring (AES) and Automated Essay Evaluation (AEE), with the latter also providing feedback to students. A complete AEC solution involves three basic stages: identification of errors, providing feedback, and grading. The evaluation criteria for essays include language proficiency, theme and genre suitability, coherence, cohesion, and use of cohesive devices. Various Brazilian models of essay correction, such as ENEM, Fuvest, and Unicamp, have their own specific criteria and grading systems.'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 3413, 'completion_tokens': 151, 'total_tokens': 3564}}\n"
          ]
        }
      ]
    }
  ]
}